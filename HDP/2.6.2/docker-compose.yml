version: '2'

services:
  master:
    build:
      context: .
      dockerfile: ./Dockerfile.cluster
    image: my/hdp-cluster
    hostname: hdp-spark-master
    container_name: my-hdp-spark-master
    ports:
      - "8040:8040"
      - "8088:8088"
      - "9000:9000"
      - "19888:19888"
      - "50070:50070"
    mem_limit: 1g
    networks:
      hadoop_nw:
        ipv4_address: 172.30.0.2
    extra_hosts:
      - "hdp-spark-worker:172.30.0.3"
    environment:
      TAG: MASTER

  worker:
    build:
      context: .
      dockerfile: ./Dockerfile.cluster
    image: my/hdp-cluster
    hostname: hdp-spark-worker
    container_name: my-hdp-spark-worker
    ports:
      - "45454:45454"
      - "50010:50010"
    mem_limit: 2g
    networks:
      hadoop_nw:
        ipv4_address: 172.30.0.3
    extra_hosts:
      - "hdp-spark-master:172.30.0.2"
    environment:
      TAG: WORKER

  client:
    build:
      context: .
      dockerfile: ./Dockerfile.client
    image: my/hdp-client
    hostname: hdp-spark-client
    container_name: my-hdp-spark-client
    ports:
      - "8080:8080"
    mem_limit: 1g
    networks:
      hadoop_nw:
        ipv4_address: 172.30.0.10
    extra_hosts:
      - "hdp-spark-master:172.30.0.2"
      - "hdp-spark-worker:172.30.0.3"
    environment:
      TAG: CLIENT
    command: tail -F anything

networks:
  hadoop_nw:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.30.0.0/16
          gateway: 172.30.0.254

